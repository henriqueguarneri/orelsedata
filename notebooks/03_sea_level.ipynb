{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91b40e6",
   "metadata": {},
   "source": [
    "# Sea Surface Level \n",
    "Notebook environment to migrate netcdf files to CF compliant zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aef40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional; code formatter, installed as jupyter lab extension\n",
    "#%load_ext lab_black\n",
    "# Optional; code formatter, installed as jupyter notebook extension\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f67518",
   "metadata": {},
   "source": [
    "### Configure OS independent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b089ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import numpy.ma as ma\n",
    "\n",
    "# Make root directories importable by appending root to path\n",
    "cwd = pathlib.Path().resolve()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "\n",
    "# Get root paths\n",
    "home = pathlib.Path().home()\n",
    "root = home.root\n",
    "\n",
    "# Import custom functionality\n",
    "from etl import p_drive\n",
    "from etl.CF_compliancy_checker import check_compliancy, save_compliancy\n",
    "\n",
    "# Define (local and) remote drives\n",
    "coclico_data_dir = p_drive.joinpath(\"11205479-coclico\", \"FASTTRACK_DATA\")\n",
    "\n",
    "# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\n",
    "os.environ[\"UDUNITS2_XML_PATH\"] = str(\n",
    "    home.joinpath(  # change to the udunits2.xml file dir in your Python installation\n",
    "        r\"Anaconda3\\pkgs\\udunits2-2.2.28-h892ecd3_0\\Library\\share\\udunits\\udunits2.xml\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef15534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths & files (manual input)\n",
    "ds_dir = coclico_data_dir.joinpath(\"03_sea_level_jrc\")\n",
    "ds_historical_path = ds_dir.joinpath(\"CoastAlRisk_Europe_ESL_Historical.nc\")\n",
    "ds_rcp45_path = ds_dir.joinpath(\"CoastAlRisk_Europe_ESL_RCP45.nc\")\n",
    "ds_rcp85_path = ds_dir.joinpath(\"CoastAlRisk_Europe_ESL_RCP85.nc\")\n",
    "ds_out_file = \"CoastAlRisk_Europe_ESL\"\n",
    "CF_dir = coclico_data_dir.joinpath(r\"CF\")  # directory to save output CF check files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd349bc5",
   "metadata": {},
   "source": [
    "### change NetCDF base files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the historical dataset to merge into the RCP45 and RCP85 one\n",
    "dataset_new = \"CoastAlRisk_Europe_ESL_Historical_new.nc\"\n",
    "ds_new = nc.Dataset(os.path.join(ds_dir, dataset_new), \"w\")\n",
    "\n",
    "# copy global attributes all at once via dictionary\n",
    "ds = nc.Dataset(ds_historical_path)\n",
    "ds_new.setncatts(ds.__dict__)\n",
    "\n",
    "# copy dimensions\n",
    "for name, dimension in ds.dimensions.items():\n",
    "    ds_new.createDimension(\n",
    "        name, (len(dimension) if not dimension.isunlimited() else None)\n",
    "    )\n",
    "\n",
    "# adding a new dimension\n",
    "ds_new.createDimension(\"nsdec\", (1))  # only for 1995\n",
    "\n",
    "# copy all file data and extend for 2 instances\n",
    "extended_var = [\"esl\", \"eewl\"]\n",
    "for name, variable in ds.variables.items():\n",
    "    if name in extended_var:\n",
    "        ds_new.createVariable(\n",
    "            name, variable.datatype, (\"npoints\", \"nrp\", \"nens\", \"nsdec\")\n",
    "        )\n",
    "        ds_new[name].setncatts(\n",
    "            ds[name].__dict__\n",
    "        )  # copy variable attributes all at once via dictionary\n",
    "        ds_new[name][:] = np.expand_dims(ds[name][:], axis=3)  # data, extended\n",
    "    else:\n",
    "        ds_new.createVariable(name, variable.datatype, variable.dimensions)\n",
    "        ds_new[name].setncatts(\n",
    "            ds[name].__dict__\n",
    "        )  # copy variable attributes all at once via dictionary\n",
    "        ds_new[name][:] = ds[name][:]  # data\n",
    "\n",
    "# adding a new variables\n",
    "ds_new.createVariable(\"decades\", \"float32\", (\"nsdec\",))\n",
    "ds_new[\"decades\"][:] = ma.masked_array(int(1995), mask=[0], dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the datasets\n",
    "ens_list_45 = [\n",
    "    xr.open_mfdataset(os.path.join(str(ds_historical_path).split(\".\")[0] + \"_new.nc\")),\n",
    "    xr.open_mfdataset(ds_rcp45_path),\n",
    "]\n",
    "ds_comb_45 = xr.concat(\n",
    "    ens_list_45, data_vars=\"different\", dim=\"nsdec\"\n",
    ")  # only concat files that are different\n",
    "\n",
    "ens_list_85 = [\n",
    "    xr.open_mfdataset(os.path.join(str(ds_historical_path).split(\".\")[0] + \"_new.nc\")),\n",
    "    xr.open_mfdataset(ds_rcp85_path),\n",
    "]\n",
    "ds_comb_85 = xr.concat(\n",
    "    ens_list_85, data_vars=\"different\", dim=\"nsdec\"\n",
    ")  # only concat files that are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811933d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the new datasets\n",
    "ds_comb_45.to_netcdf(\n",
    "    os.path.join(str(ds_rcp45_path).split(\".\")[0] + \"_new.nc\")\n",
    ")  # Export netcdf file\n",
    "ds_comb_85.to_netcdf(\n",
    "    os.path.join(str(ds_rcp85_path).split(\".\")[0] + \"_new.nc\")\n",
    ")  # Export netcdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d1359",
   "metadata": {},
   "source": [
    "### Check CF compliancy original NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f3596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite paths to load new files\n",
    "ds_historical_path = ds_dir.joinpath(\"CoastAlRisk_Europe_ESL_Historical_new.nc\")\n",
    "ds_rcp45_path = ds_dir.joinpath(\"CoastAlRisk_Europe_ESL_RCP45_new.nc\")\n",
    "ds_rcp85_path = ds_dir.joinpath(\"CoastAlRisk_Europe_ESL_RCP85_new.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets\n",
    "ds_hist = xr.open_dataset(ds_historical_path)\n",
    "ds_45rcp = xr.open_dataset(ds_rcp45_path)\n",
    "ds_85rcp = xr.open_dataset(ds_rcp85_path)\n",
    "\n",
    "# check original dataset\n",
    "ds_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4057296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_historical_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_historical_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f569418",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_rcp45_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_rcp45_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_rcp85_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ae0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_rcp85_path, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9150b",
   "metadata": {},
   "source": [
    "### Make CF compliant alterations to the NetCDF files (dataset dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aebd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetCDF attribute alterations\n",
    "\n",
    "# rename global attribute names\n",
    "ds_hist.attrs[\"Project_Name\"] = ds_hist.attrs.pop(\"Project Name\")\n",
    "ds_hist.attrs[\"Project_Acronym\"] = ds_hist.attrs.pop(\"Project Acronym\")\n",
    "ds_45rcp.attrs[\"Project_Name\"] = ds_45rcp.attrs.pop(\"Project Name\")\n",
    "ds_45rcp.attrs[\"Project_Acronym\"] = ds_45rcp.attrs.pop(\"Project Acronym\")\n",
    "ds_85rcp.attrs[\"Project_Name\"] = ds_85rcp.attrs.pop(\"Project Name\")\n",
    "ds_85rcp.attrs[\"Project_Acronym\"] = ds_85rcp.attrs.pop(\"Project Acronym\")\n",
    "\n",
    "# add global attributes\n",
    "ds_hist.attrs[\"Conventions\"] = \"CF-1.8\"\n",
    "ds_45rcp.attrs[\"Conventions\"] = \"CF-1.8\"\n",
    "ds_85rcp.attrs[\"Conventions\"] = \"CF-1.8\"\n",
    "ds_hist.attrs[\"Starting_date\"] = \"01-Dec-1969\"\n",
    "ds_hist.attrs[\"End_date\"] = \"30-Nov-2004 21:00:00\"\n",
    "ds_45rcp.attrs[\"Starting_date\"] = \"01-Dec-2009\"\n",
    "ds_45rcp.attrs[\"End_date\"] = \"30-Nov-2099 21:00:00\"\n",
    "ds_85rcp.attrs[\"Starting_date\"] = \"01-Dec-2009\"\n",
    "ds_85rcp.attrs[\"End_date\"] = \"30-Nov-2099 21:00:00\"\n",
    "\n",
    "# remove certain variable attributes\n",
    "del ds_hist[\"rp\"].attrs[\"Starting date\"]\n",
    "del ds_hist[\"rp\"].attrs[\"End date\"]\n",
    "del ds_45rcp[\"rp\"].attrs[\"Starting date\"]\n",
    "del ds_45rcp[\"rp\"].attrs[\"End date\"]\n",
    "del ds_85rcp[\"rp\"].attrs[\"Starting date\"]\n",
    "del ds_85rcp[\"rp\"].attrs[\"End date\"]\n",
    "\n",
    "# add or change certain variable / coordinate attributes\n",
    "dataset_attributes = {\n",
    "    \"decades\": {\"long_name\": \"decade window\", \"Format\": \"YYYY\", \"units\": \"yr\"}\n",
    "}  # specify custom (CF convention) attributes\n",
    "\n",
    "# add / overwrite attributes\n",
    "for k, v in dataset_attributes.items():\n",
    "    try:\n",
    "        ds_hist[k].attrs = dataset_attributes[k]\n",
    "        ds_45rcp[k].attrs = dataset_attributes[k]\n",
    "        ds_85rcp[k].attrs = dataset_attributes[k]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41053533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetCDF variable and dimension alterations\n",
    "\n",
    "# rename or swap dimension names, the latter in case the name already exists as coordinate\n",
    "ds_hist = ds_hist.rename_dims(\n",
    "    {\"npoints\": \"stations\", \"nens\": \"nensemble\", \"nsdec\": \"time\"}\n",
    ")\n",
    "ds_45rcp = ds_45rcp.rename_dims(\n",
    "    {\"npoints\": \"stations\", \"nens\": \"nensemble\", \"nsdec\": \"time\"}\n",
    ")\n",
    "ds_85rcp = ds_85rcp.rename_dims(\n",
    "    {\"npoints\": \"stations\", \"nens\": \"nensemble\", \"nsdec\": \"time\"}\n",
    ")\n",
    "ds_hist = ds_hist.swap_dims({\"nrp\": \"rp\"})\n",
    "ds_45rcp = ds_45rcp.swap_dims({\"nrp\": \"rp\"})\n",
    "ds_85rcp = ds_85rcp.swap_dims({\"nrp\": \"rp\"})\n",
    "\n",
    "# rename variables, if necessary\n",
    "ds_hist = ds_hist.rename_vars(\n",
    "    {\"longitude\": \"lon\", \"latitude\": \"lat\", \"ensmbl\": \"ensemble\", \"decades\": \"time\"}\n",
    ")\n",
    "ds_45rcp = ds_45rcp.rename_vars(\n",
    "    {\"longitude\": \"lon\", \"latitude\": \"lat\", \"ensmbl\": \"ensemble\", \"decades\": \"time\"}\n",
    ")\n",
    "ds_85rcp = ds_85rcp.rename_vars(\n",
    "    {\"longitude\": \"lon\", \"latitude\": \"lat\", \"ensmbl\": \"ensemble\", \"decades\": \"time\"}\n",
    ")\n",
    "\n",
    "# set some data variables to coordinates to avoid duplication of dimensions in later stage\n",
    "ds_hist = ds_hist.set_coords([\"lon\", \"lat\", \"rp\", \"time\"])\n",
    "ds_45rcp = ds_45rcp.set_coords([\"lon\", \"lat\", \"rp\", \"time\"])\n",
    "ds_85rcp = ds_85rcp.set_coords([\"lon\", \"lat\", \"rp\", \"time\"])\n",
    "\n",
    "# assign coordinate strings i.s.o. integers (and copy along the attributes)\n",
    "ds_hist = ds_hist.assign_coords(\n",
    "    {\n",
    "        \"ensemble\": (\n",
    "            \"nensemble\",\n",
    "            np.array([\"min\", \"mean\", \"max\"], dtype=\"S\"),\n",
    "            ds_hist.ensemble.attrs,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "ds_45rcp = ds_45rcp.assign_coords(\n",
    "    {\n",
    "        \"ensemble\": (\n",
    "            \"nensemble\",\n",
    "            np.array([\"min\", \"mean\", \"max\"], dtype=\"S\"),\n",
    "            ds_45rcp.ensemble.attrs,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "ds_85rcp = ds_85rcp.assign_coords(\n",
    "    {\n",
    "        \"ensemble\": (\n",
    "            \"nensemble\",\n",
    "            np.array([\"min\", \"mean\", \"max\"], dtype=\"S\"),\n",
    "            ds_85rcp.ensemble.attrs,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# remove attributes in copied entries\n",
    "del ds_hist[\"ensemble\"].attrs[\"Contents\"]\n",
    "del ds_45rcp[\"ensemble\"].attrs[\"Contents\"]\n",
    "del ds_85rcp[\"ensemble\"].attrs[\"Contents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f9950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat datasets along new dimension with index values and name derived from pandas index object, if necessary\n",
    "dataset = xr.concat([ds_45rcp, ds_85rcp], dim=\"nscenarios\")\n",
    "dataset = dataset.assign_coords(\n",
    "    scenarios=(\"nscenarios\", np.array([\"RCP45\", \"RCP85\"], dtype=\"S\"))\n",
    ")\n",
    "\n",
    "# dataset = xr.concat(\n",
    "#     [dataset_historical, dataset_45rcp, dataset_85rcp],\n",
    "#     pd.Index([\"historical\", \"rcp45\", \"rcp85\"], name=\"scenarios\"),\n",
    "# )\n",
    "\n",
    "# dataset[\"scenarios\"].values.astype(\"U\") # retrieve scenarios as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order shape of the data variables\n",
    "ds_hist = ds_hist.transpose(\"stations\", \"rp\", \"time\", \"nensemble\")\n",
    "ds_45rcp = ds_45rcp.transpose(\"stations\", \"rp\", \"time\", \"nensemble\")\n",
    "ds_85rcp = ds_85rcp.transpose(\"stations\", \"rp\", \"time\", \"nensemble\")\n",
    "dataset = dataset.transpose(\"nscenarios\", \"stations\", \"rp\", \"time\", \"nensemble\")\n",
    "\n",
    "# add or change certain variable / coordinate attributes\n",
    "dataset_attributes = {\n",
    "    \"scenarios\": {\"long_name\": \"climate scenarios\"}\n",
    "}  # specify custom (CF convention) attributes\n",
    "\n",
    "# add / overwrite attributes\n",
    "for k, v in dataset_attributes.items():\n",
    "    try:\n",
    "        dataset[k].attrs = dataset_attributes[k]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# add epsg\n",
    "ds_hist.attrs[\"crs\"] = 4326\n",
    "ds_45rcp.attrs[\"crs\"] = 4326\n",
    "ds_85rcp.attrs[\"crs\"] = 4326\n",
    "dataset.attrs[\"crs\"] = 4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47662719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the xarray dataset, best practice is to have as many as possible bold dimensions (dimension == coordinate name).\n",
    "# in this way, the Front-End can access the variable directly without having to index the variable first\n",
    "\n",
    "dataset\n",
    "# dataset[\"scenarios\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ebbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new .nc files\n",
    "ds_hist.to_netcdf(path=str(ds_historical_path).replace(\".nc\", \"_CF.nc\"))\n",
    "ds_45rcp.to_netcdf(path=str(ds_rcp45_path).replace(\".nc\", \"_CF.nc\"))\n",
    "ds_85rcp.to_netcdf(path=str(ds_rcp85_path).replace(\".nc\", \"_CF.nc\"))\n",
    "dataset.to_netcdf(path=ds_dir.joinpath(ds_out_file + \"_CF.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeedd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(str(ds_historical_path).replace(\".nc\", \"_new_CF.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77110cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"ensemble\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7b37d",
   "metadata": {},
   "source": [
    "### Check CF compliancy altered NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f992f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_historical_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ba94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_historical_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_rcp45_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc254b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_rcp45_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef9525",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_rcp85_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_rcp85_path).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb764dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037decf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec081e",
   "metadata": {},
   "source": [
    "### write data to Zarr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to zarr in write mode (to overwrite if exists)\n",
    "dataset.to_zarr(ds_dir.joinpath(\"%s.zarr\" % ds_out_file), mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce96dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d52b8dfbdab1c939c3c4b10b0d762f4c8139583e350f28e123ee37db8f80dd50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
