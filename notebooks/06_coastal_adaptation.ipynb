{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa33a9ce-c5ee-455d-85d8-b75f2a44ee31",
   "metadata": {},
   "source": [
    "# Cost and Benefit Coastal Adaptation\n",
    "\n",
    "Notebook environment to migrate netcdf files to zarr and geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c25cd-7a92-4123-b7ad-26468459bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the black code formatter\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0662f-d6ca-4fd2-97fe-3806ff4f90e9",
   "metadata": {},
   "source": [
    "### Configure OS independent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccd64b-b885-4bac-b157-90035272c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# Make root directories importable by appending root to path\n",
    "cwd = pathlib.Path().resolve()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "\n",
    "\n",
    "# Get root paths\n",
    "home = pathlib.Path().home()\n",
    "root = home.root\n",
    "\n",
    "# Define both local and remote drives\n",
    "local_data_dir = home.joinpath(\"ddata\")\n",
    "local_temp_dir = local_data_dir.joinpath(\"tmp\")\n",
    "p_dir = pathlib.Path(root, \"p\")\n",
    "coclico_data_dir = p_dir.joinpath(\"11205479-coclico\", \"data\")\n",
    "coclico_cf_dir = coclico_data_dir.joinpath(\"CF\")\n",
    "ds_dirname = \"06_adaptation_jrc\"\n",
    "\n",
    "# Project paths\n",
    "local_auth_dir = local_data_dir.joinpath(\"AUTH_files\")\n",
    "remote_auth_dir = coclico_data_dir.joinpath(\"AUTH_files\")\n",
    "netcdf_dir = pathlib.Path(\"netcdf_files\", \"06.Coast and benefits of coastal adaptation\")\n",
    "json_dir = pathlib.Path(\"json_files\", \"06.Coast and benefits of coastal adaptation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d018d0-d37c-4974-aede-9ff3de0addc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760f01d-247a-4997-81c3-bd61f783c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp(fn, suffix, remote_drive=True):\n",
    "    file_dirs = {\n",
    "        \".json\": pathlib.Path(\n",
    "            \"json_files\", \"06.Coast and benefits of coastal adaptation\"\n",
    "        ),\n",
    "        \".nc\": pathlib.Path(\n",
    "            \"netcdf_files\", \"06.Coast and benefits of coastal adaptation\"\n",
    "        ),\n",
    "    }\n",
    "    local_auth_dir = local_data_dir.joinpath(\"tmp\", \"AUTH_files\")\n",
    "    remote_auth_dir = coclico_data_dir.joinpath(\"temp\", \"AUTH_files\")\n",
    "\n",
    "    if not remote_drive:\n",
    "        return local_auth_dir.joinpath(file_dirs[suffix]).joinpath(fn + suffix)\n",
    "    return remote_auth_dir.joinpath(file_dirs[suffix]).joinpath(fn + suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59975c47-72fc-449e-92d7-273f9ebea8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_benefit = \"benefitNoDiscounting\"\n",
    "fn_cost = \"costNoDiscounting\"\n",
    "fn_cbr = \"cbr\"\n",
    "fn_protection = \"dZprotectionMean\"\n",
    "\n",
    "files = [fn_benefit, fn_cost, fn_cbr, fn_protection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03312ba0-9b9b-4f6d-8665-88eed3117563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_benefit, ds_cost, ds_cbr, ds_protection = [\n",
    "    xr.load_dataset(get_fp(fn, suffix=\".nc\", remote_drive=False)) for fn in files\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a4ec3-d9fe-4a44-aed7-f38ad31a4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benefit, df_cost, df_cbr, df_protection = [\n",
    "    pd.read_json(get_fp(fn, suffix=\".json\", remote_drive=False)) for fn in files\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017728d6-9c89-458a-8f8c-5927f8fdbf1b",
   "metadata": {},
   "source": [
    "### Load in raw data from p drive (excel sheets)\n",
    "\n",
    "The nuts regions are not included as attributes in the netcdf files. The ones from the excel sheet are not present in recent nuts regsion shapefile by the EU. Therefore, project coordinates from data into current nuts regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d646e-9c55-4138-9cc9-20be0cbba23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_benefit, xlsx_cost, xlsx_cbr, xlsx_protection = [\n",
    "    pd.read_excel(local_temp_dir.joinpath(\"06_adaptation_jrc\", f\"{fn}.xlsx\"))\n",
    "    for fn in files\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec47579-19a2-4f89-82e7-38d2d88ae828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "xlsx_dfs = xlsx_benefit, xlsx_cost, xlsx_cbr, xlsx_protection\n",
    "xlsx_merged = reduce(\n",
    "    lambda l, r: pd.merge(l, r, on=[\"NUTS2 ID\"], how=\"outer\"), xlsx_dfs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85055e6-8e63-4b3e-a553-a6615b13ff2c",
   "metadata": {},
   "source": [
    "### Add nuts region\n",
    "\n",
    "Nuts regions are obtained from eurostat, but the most recent nuts regions files do not\n",
    "match the ones which are used in the datasets. The files describing the 2010 nuts regions\n",
    "seem to match with the regions used in the studies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49f8bd-ccf2-4bda-8e19-bdf53bb13f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_regions = gpd.read_file(\n",
    "    local_data_dir.joinpath(\"tmp\", \"NUTS_RG_20M_2010_3857.shp\")\n",
    ")\n",
    "nuts_regions = nuts_regions.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use one of the datasets to create a geodataframe\n",
    "df_cost = df_cost.rename(\n",
    "    {\n",
    "        \"latitude(degrees north of the NUTS2 regions centroid)\": \"latitude\",\n",
    "        \"longitude(degrees east of the NUTS2 regions centroid)\": \"longitude\",\n",
    "    },\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "gdf_cost = gpd.GeoDataFrame(\n",
    "    df_cost,\n",
    "    geometry=gpd.points_from_xy(df_cost.longitude, df_cost.latitude),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "# Add nuts column from excel data\n",
    "gdf_cost[\"NUTS_ID\"] = xlsx_cost[\"NUTS2 ID\"]\n",
    "\n",
    "# inner join to keep only nuts regions used in dataset\n",
    "nuts_regions = nuts_regions.merge(gdf_cost, on=[\"NUTS_ID\"], how=\"inner\")\n",
    "\n",
    "# format dataframe\n",
    "nuts_regions[\"instance\"] = nuts_regions.index.values\n",
    "nuts_regions = nuts_regions[\n",
    "    [\"instance\", \"NUTS_ID\", \"NAME_LATN\", \"CNTR_CODE\", \"geometry_x\"]\n",
    "]\n",
    "nuts_regions = nuts_regions.rename(\n",
    "    {\n",
    "        \"NUTS_ID\": \"acronym\",\n",
    "        \"NAME_LATN\": \"name\",\n",
    "        \"CNTR_CODE\": \"country\",\n",
    "        \"geometry_x\": \"geometry\",\n",
    "    },\n",
    "    axis=\"columns\",\n",
    ")\n",
    "nuts_regions = gpd.GeoDataFrame(nuts_regions, crs=\"EPSG:4326\")\n",
    "nuts_regions.head()\n",
    "\n",
    "# write result to geojson\n",
    "# nuts_regions.to_file(\n",
    "#     coclico_data_dir.joinpath(\"06_adaptation_jrc\", \"nuts_regions.geojson\"),\n",
    "#     driver=\"GeoJSON\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1639c9-c6f7-49ec-96cf-5babad7ad31b",
   "metadata": {},
   "source": [
    "## Make datasets CF compliant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22d1a7-6e1b-4ee8-89e1-f1a06e5b6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set lon/lat coordinates for each of the datasets\n",
    "ds_benefit, ds_cost, ds_cbr, ds_protection = [\n",
    "    ds.set_coords([\"lon\", \"lat\"]) for ds in [ds_benefit, ds_cost, ds_cbr, ds_protection]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a0a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dataset(ds: xr.Dataset, var_name: str) -> xr.Dataset:\n",
    "    \"\"\"Store variables in dimension scenarios.\n",
    "\n",
    "    Dataset contains multiple variables which each represent a combined RCP-SSP scenario.\n",
    "    These are extracted and stored in one common dimension scenarios.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): _description_\n",
    "        var_name (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: _description_\n",
    "    \"\"\"\n",
    "    ds_ = ds.copy()\n",
    "    ds_ = (\n",
    "        ds_.to_array(\"nscenarios\", var_name)\n",
    "        .to_dataset()\n",
    "        .reset_index(\"nscenarios\", drop=True)\n",
    "        .assign_coords(\n",
    "            scenarios=(\"nscenarios\", np.array([\"RCP45-SSP1\", \"RCP85-SSP5\"], dtype=\"S\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ds_ = ds_.rename_dims({\"row\": \"stations\"})\n",
    "    ds_ = ds_.transpose(\"nscenarios\", \"stations\")\n",
    "    return ds_\n",
    "\n",
    "\n",
    "ds_benefit_ = reshape_dataset(ds_benefit, \"benefit\")\n",
    "ds_cost_ = reshape_dataset(ds_cost, \"cost\")\n",
    "ds_cbr_ = reshape_dataset(ds_cbr, \"cbr\")\n",
    "ds_protection_ = reshape_dataset(ds_protection, \"eb\")\n",
    "ds = xr.merge([ds_benefit_, ds_cost_, ds_cbr_, ds_protection_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9437449",
   "metadata": {},
   "source": [
    "### Add geometries from NUTS regions as coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkb\n",
    "\n",
    "# extract geometries of nut2 regions in well-known binary format\n",
    "geoms = nuts_regions[\"geometry\"].apply(lambda x: wkb.dumps(x))\n",
    "\n",
    "# rename dims and add new data to dataset\n",
    "ds = ds.assign_coords({\"geometry\": (\"stations\", geoms)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add global attrs\n",
    "ds.attrs[\"Conventions\"] = \"CF-1.8\"\n",
    "ds.attrs[\"crs\"] = 4326\n",
    "\n",
    "# add coordinates attributes\n",
    "ds[\"lon\"].attrs[\"standard_name\"] = \"longitude\"\n",
    "ds[\"lon\"].attrs[\"units\"] = \"degrees_east\"\n",
    "ds[\"lon\"].attrs[\n",
    "    \"long_name\"\n",
    "] = \"Longitude of the centroid of the NUTS2 region (2010 version).\"\n",
    "del ds[\"lon\"].attrs[\"_CoordinateAxisType\"]\n",
    "\n",
    "ds[\"lat\"].attrs[\"standard_name\"] = \"latitude\"\n",
    "ds[\"lat\"].attrs[\"units\"] = \"degrees_north\"\n",
    "ds[\"lat\"].attrs[\n",
    "    \"long_name\"\n",
    "] = \"Latitude of the centroid of the NUTS2 region (2010 version).\"\n",
    "del ds[\"lat\"].attrs[\"_CoordinateAxisType\"]\n",
    "\n",
    "ds[\"geometry\"].attrs = {\n",
    "    \"long_name\": \"NUTS2 regions (polygons) in well-known binary format (wkb).\",\n",
    "    \"geometry_type\": \"polygon\",\n",
    "    \"units\": \"degree\",\n",
    "    \"comment\": \"These NUTS2 regions (2010 version) are available at Eurostat.\",\n",
    "    \"crs_wkt\": f\"{nuts_regions.crs.to_epsg()}\",\n",
    "}\n",
    "\n",
    "ds[\"scenarios\"].attrs = {\"long_name\": \"Combined RCP and SSP climate scenarios.\"}\n",
    "\n",
    "\n",
    "# add variable attributes\n",
    "ds[\"benefit\"].attrs = {\"long_name\": ds_benefit.attrs[\"title\"], \"units\": \"EUR 1 000 000\"}\n",
    "ds[\"cost\"].attrs = {\"long_name\": ds_cost.attrs[\"title\"], \"units\": \"EUR 1 000 000\"}\n",
    "ds[\"cbr\"].attrs = {\"long_name\": ds_benefit.attrs[\"title\"]}\n",
    "ds[\"eb\"].attrs = {\"long_name\": ds_protection.attrs[\"title\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeaa4af",
   "metadata": {},
   "source": [
    "### Run CF checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save current dataset as netcdf in tmp directory\n",
    "ds_outpath = local_temp_dir.joinpath(\"cbca_CF.nc\")\n",
    "ds.to_netcdf(path=ds_outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check using cfecker python library (default settings, hence, most current var, region, ..., etc. names)\n",
    "from cfchecker.cfchecks import CFChecker\n",
    "\n",
    "CFChecker().checker(str(ds_outpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef42b8",
   "metadata": {},
   "source": [
    "### Write CF logs to p_drive as backlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths to save log files\n",
    "\n",
    "cf_dir = coclico_cf_dir.joinpath(ds_dirname)\n",
    "if not cf_dir.exists():\n",
    "    cf_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "# write CF logs to p_drive\n",
    "with open(cf_dir.joinpath(ds_outpath.stem).with_suffix(\".check\"), \"w\") as f:\n",
    "    with redirect_stdout(f):\n",
    "        CFChecker().checker(str(ds_outpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bb63b",
   "metadata": {},
   "source": [
    "### Copy files from local to p_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50359df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# #TODO: fix permission error when copying to p_drive\n",
    "# shutil.copy(ds_outpath, coclico_data_dir.joinpath(ds_dirname, ds_outpath.name))\n",
    "\n",
    "# workaround: print cp command to use in shell\n",
    "print(f\"cp '{ds_outpath}' '{coclico_data_dir.joinpath(ds_dirname, ds_outpath.name)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b6a40",
   "metadata": {},
   "source": [
    "# Save to zarr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680681df",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_store_fp = local_temp_dir.joinpath(\"eu_coastal_adaptation.zarr\")\n",
    "ds.to_zarr(zarr_store_fp, mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cd99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"/p/11205479-coclico/data/06_adaptation_jrc/cbca_CF.nc\").exists()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('bathymetry_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7824e7e51848de437b58b04c6b1a3326349c0c0f9739c0b57d35806ea1309c20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
