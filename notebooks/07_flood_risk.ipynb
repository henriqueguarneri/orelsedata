{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa33a9ce-c5ee-455d-85d8-b75f2a44ee31",
   "metadata": {},
   "source": [
    "# Coastal Flood Risk\n",
    "\n",
    "Notebook environment to migrate netcdf files to zarr and geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c25cd-7a92-4123-b7ad-26468459bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional; code formatter, installed as jupyter lab extension\n",
    "#%load_ext lab_black\n",
    "# Optional; code formatter, installed as jupyter notebook extension\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0662f-d6ca-4fd2-97fe-3806ff4f90e9",
   "metadata": {},
   "source": [
    "### Configure OS independent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91597999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import numpy.ma as ma\n",
    "import math\n",
    "from shapely import wkb\n",
    "\n",
    "# Make root directories importable by appending root to path\n",
    "cwd = pathlib.Path().resolve()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "\n",
    "# Get root paths\n",
    "home = pathlib.Path().home()\n",
    "root = home.root\n",
    "\n",
    "# Import custom functionality\n",
    "from etl import p_drive\n",
    "from etl.CF_compliancy_checker import check_compliancy, save_compliancy\n",
    "\n",
    "# Define (local and) remote drives\n",
    "coclico_data_dir = p_drive.joinpath(\"11205479-coclico\", \"data\")\n",
    "\n",
    "# Workaround to the Windows OS (10) udunits error after installation of cfchecker: https://github.com/SciTools/iris/issues/404\n",
    "os.environ[\"UDUNITS2_XML_PATH\"] = str(\n",
    "    home.joinpath(  # change to the udunits2.xml file dir in your Python installation\n",
    "        r\"Anaconda3\\pkgs\\udunits2-2.2.28-h892ecd3_0\\Library\\share\\udunits\\udunits2.xml\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths & files (manual input)\n",
    "ds_dir = coclico_data_dir.joinpath(\"07_flood_risk_jrc\")\n",
    "ds_path1 = ds_dir.joinpath(\"ExpectedAnnualDamage.nc\")\n",
    "ds_path2 = ds_dir.joinpath(\"ExpectedAnnualDamageperGDP.nc\")\n",
    "ds_path3 = ds_dir.joinpath(\"ExpectedAnnualPeopleAffected.nc\")\n",
    "ds_out_file = \"Coastal_Flood_risk_Europe\"\n",
    "CF_dir = coclico_data_dir.joinpath(r\"CF\")  # directory to save output CF check files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b294f1",
   "metadata": {},
   "source": [
    "### Check CF compliancy original NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets\n",
    "ds1 = xr.open_dataset(ds_path1)\n",
    "ds2 = xr.open_dataset(ds_path2)\n",
    "ds3 = xr.open_dataset(ds_path3)\n",
    "\n",
    "# check original dataset\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_path1, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04751f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_path1, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_path2, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_path2, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e199d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check original CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_path3, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original CF compliancy\n",
    "save_compliancy(cap, testfile=ds_path3, working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e16687",
   "metadata": {},
   "source": [
    "### Add NUTS regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045dd46",
   "metadata": {},
   "source": [
    "The nuts regions are not included as attributes in the netCDF files. The NetCDF files only contain lon and lat values at the centroid of NUTS2 regions so we retrieve that information from eurostat (\"https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nuts regions, filter on countries and replace the geomtry by buffered centroids\n",
    "nuts_regions = gpd.read_file(\n",
    "    coclico_data_dir.joinpath(\"XX_NUTS2\", \"NUTS_RG_20M_2021_4326.geojson\")\n",
    ")\n",
    "nuts_regions = nuts_regions.to_crs(\"EPSG:4326\")\n",
    "\n",
    "NUTS0 = nuts_regions[nuts_regions.LEVL_CODE == 0]  # countries\n",
    "NUTS0[\"polygons\"] = NUTS0.geometry  # rename the geometry\n",
    "NUTS0[\"geometry\"] = NUTS0.centroid.buffer(0.5)  # set the centroid as geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ce632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy file from xarray dataset with lon and lat centroids\n",
    "ds_d = gpd.GeoDataFrame(geometry=gpd.points_from_xy(ds1.lon, ds1.lat), crs=\"EPSG:4326\",)\n",
    "\n",
    "# spatial join to keep only nuts regions used ds1\n",
    "sjoins = gpd.sjoin(ds_d, NUTS0, how=\"left\")\n",
    "\n",
    "# spatial join alterations (i.e. renaming the columns and reducing the size)\n",
    "sjoins[\"instance\"] = sjoins.index_right.values\n",
    "cropped_list = [\"instance\", \"NUTS_ID\", \"NAME_LATN\", \"CNTR_CODE\", \"polygons\"]\n",
    "renamed_list = [\"instance\", \"acronym\", \"name\", \"country\", \"geometry\"]\n",
    "sjoins = sjoins[cropped_list]\n",
    "sjoins = sjoins.rename(\n",
    "    {\n",
    "        \"NUTS_ID\": renamed_list[1],\n",
    "        \"NAME_LATN\": renamed_list[2],\n",
    "        \"CNTR_CODE\": renamed_list[3],\n",
    "        \"polygons\": renamed_list[4],\n",
    "    },\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "# supplementing the faulty joins\n",
    "supplement = [\"FR\", \"NO\"]\n",
    "nan_idx = []\n",
    "for idx, i in enumerate(sjoins.instance):\n",
    "    if math.isnan(i) == True:\n",
    "        nan_idx.append(idx)\n",
    "\n",
    "for i, j in zip(nan_idx, supplement):\n",
    "    supp = NUTS0.loc[NUTS0[\"NUTS_ID\"] == j]\n",
    "    for kid, (k, l) in enumerate(zip(renamed_list, cropped_list)):\n",
    "        if kid == 0:\n",
    "            sjoins.at[i, k] = supp.index.values[0]\n",
    "        else:\n",
    "            sjoins.at[i, k] = supp[l].values[0]\n",
    "\n",
    "sjoins = gpd.GeoDataFrame(sjoins, crs=\"EPSG:4326\")\n",
    "sjoins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add geometries\n",
    "\n",
    "# extract geometries of nut2 regions in well-known binary format\n",
    "geoms = sjoins[\"geometry\"].apply(lambda x: wkb.dumps(x))\n",
    "\n",
    "# rename dims and add new data to dataset\n",
    "ds1 = ds1.assign_coords({\"geometry\": (\"row\", geoms)})\n",
    "ds2 = ds2.assign_coords({\"geometry\": (\"row\", geoms)})\n",
    "ds3 = ds3.assign_coords({\"geometry\": (\"row\", geoms)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_geom_attrs = {\n",
    "    \"geometry\": {\n",
    "        \"long_name\": \"NUTS2 regions (polygons) in well-known binary format (wkb).\",\n",
    "        \"geometry_type\": \"polygon\",\n",
    "        \"units\": \"degree\",\n",
    "        \"comment\": \"These NUTS2 regions (2021 version) are available at Eurostat.\",\n",
    "        \"crs_wkt\": f\"{sjoins.crs.to_epsg()}\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for k, v in add_geom_attrs.items():\n",
    "    ds1[k].attrs = add_geom_attrs[k]\n",
    "    ds2[k].attrs = add_geom_attrs[k]\n",
    "    ds3[k].attrs = add_geom_attrs[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5454242",
   "metadata": {},
   "source": [
    "### Make CF compliant alterations to the NetCDF files (dataset dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetCDF attribute, variable and dimension alterations\n",
    "\n",
    "# set lon/lat coordinates for each of the datasets\n",
    "ds1, ds2, ds3 = [ds.set_coords([\"lon\", \"lat\"]) for ds in [ds1, ds2, ds3]]\n",
    "\n",
    "# add global attributes\n",
    "ds1.attrs[\"Conventions\"] = \"CF-1.8\"\n",
    "ds2.attrs[\"Conventions\"] = \"CF-1.8\"\n",
    "ds3.attrs[\"Conventions\"] = \"CF-1.8\"\n",
    "ds1.attrs[\"crs\"] = 4326\n",
    "ds2.attrs[\"crs\"] = 4326\n",
    "ds3.attrs[\"crs\"] = 4326\n",
    "\n",
    "# rename dimensions\n",
    "ds1 = ds1.rename_dims({\"row\": \"stations\"})\n",
    "ds2 = ds2.rename_dims({\"row\": \"stations\"})\n",
    "ds3 = ds3.rename_dims({\"row\": \"stations\"})\n",
    "\n",
    "# alter variable attributes\n",
    "ds_list = [ds1, ds2, ds3]\n",
    "for i in range(3):\n",
    "\n",
    "    ds_list[i].lat.attrs[\"long_name\"] = ds_list[i].lat.attrs[\"standard_name\"]\n",
    "    ds_list[i].lat.attrs[\"standard_name\"] = \"latitude\"\n",
    "    ds_list[i].lat.attrs[\"units\"] = \"degrees_north\"\n",
    "\n",
    "    ds_list[i].lon.attrs[\"long_name\"] = ds_list[i].lon.attrs[\"standard_name\"]\n",
    "    ds_list[i].lon.attrs[\"standard_name\"] = \"latitude\"\n",
    "    ds_list[i].lon.attrs[\"units\"] = \"degrees_north\"\n",
    "\n",
    "    ds_list[i].base2000.attrs[\"long_name\"] = ds_list[i].base2000.attrs[\"standard_name\"]\n",
    "    del ds_list[i].base2000.attrs[\"standard_name\"]\n",
    "    ds_list[i].sust2050.attrs[\"long_name\"] = ds_list[i].sust2050.attrs[\"standard_name\"]\n",
    "    del ds_list[i].sust2050.attrs[\"standard_name\"]\n",
    "    ds_list[i].sust2100.attrs[\"long_name\"] = ds_list[i].sust2100.attrs[\"standard_name\"]\n",
    "    del ds_list[i].sust2100.attrs[\"standard_name\"]\n",
    "    ds_list[i].frag2050.attrs[\"long_name\"] = ds_list[i].frag2050.attrs[\"standard_name\"]\n",
    "    del ds_list[i].frag2050.attrs[\"standard_name\"]\n",
    "    ds_list[i].frag2100.attrs[\"long_name\"] = ds_list[i].frag2100.attrs[\"standard_name\"]\n",
    "    del ds_list[i].frag2100.attrs[\"standard_name\"]\n",
    "    ds_list[i].ffbd2050.attrs[\"long_name\"] = ds_list[i].ffbd2050.attrs[\"standard_name\"]\n",
    "    del ds_list[i].ffbd2050.attrs[\"standard_name\"]\n",
    "    ds_list[i].ffbd2100.attrs[\"long_name\"] = ds_list[i].ffbd2100.attrs[\"standard_name\"]\n",
    "    del ds_list[i].ffbd2100.attrs[\"standard_name\"]\n",
    "\n",
    "    if i == 0 or i == 2:  # annual damage\n",
    "        ds_list[i].base2000.attrs[\"units\"] = \"1e12\"\n",
    "        ds_list[i].sust2050.attrs[\"units\"], ds_list[i].sust2100.attrs[\"units\"] = (\n",
    "            \"1e12\",\n",
    "            \"1e12\",\n",
    "        )\n",
    "        ds_list[i].frag2050.attrs[\"units\"], ds_list[i].frag2100.attrs[\"units\"] = (\n",
    "            \"1e12\",\n",
    "            \"1e12\",\n",
    "        )\n",
    "        ds_list[i].ffbd2050.attrs[\"units\"], ds_list[i].ffbd2100.attrs[\"units\"] = (\n",
    "            \"1e12\",\n",
    "            \"1e12\",\n",
    "        )\n",
    "    if i == 1:  # annual damage per GDP\n",
    "        ds_list[i].base2000.attrs[\"units\"] = \"1\"\n",
    "        ds_list[i].sust2050.attrs[\"units\"], ds_list[i].sust2100.attrs[\"units\"] = (\n",
    "            \"1\",\n",
    "            \"1\",\n",
    "        )\n",
    "        ds_list[i].frag2050.attrs[\"units\"], ds_list[i].frag2100.attrs[\"units\"] = (\n",
    "            \"1\",\n",
    "            \"1\",\n",
    "        )\n",
    "        ds_list[i].ffbd2050.attrs[\"units\"], ds_list[i].ffbd2100.attrs[\"units\"] = (\n",
    "            \"1\",\n",
    "            \"1\",\n",
    "        )\n",
    "    if i == 2:  # annual people affected\n",
    "        ds_list[i].base2000.attrs[\"units\"] = \"1e3\"\n",
    "        ds_list[i].sust2050.attrs[\"units\"], ds_list[i].sust2100.attrs[\"units\"] = (\n",
    "            \"1e3\",\n",
    "            \"1e3\",\n",
    "        )\n",
    "        ds_list[i].frag2050.attrs[\"units\"], ds_list[i].frag2100.attrs[\"units\"] = (\n",
    "            \"1e3\",\n",
    "            \"1e3\",\n",
    "        )\n",
    "        ds_list[i].ffbd2050.attrs[\"units\"], ds_list[i].ffbd2100.attrs[\"units\"] = (\n",
    "            \"1e3\",\n",
    "            \"1e3\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a309c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetCDF variable and dimension alterations\n",
    "\n",
    "# concatenate the datasets for time and scenarios\n",
    "ds_list_new = []\n",
    "var_list = [\n",
    "    \"expected annual damage\",\n",
    "    \"expected annual damage per GDP\",\n",
    "    \"expected annual people affected\",\n",
    "]\n",
    "var_list_abb = [\"ead\", \"ead_GDP\", \"eapa\"]\n",
    "for i in range(3):\n",
    "    das = xr.concat(\n",
    "        [ds_list[i][\"base2000\"], ds_list[i][\"sust2050\"], ds_list[i][\"sust2100\"]],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "    das = das.assign_coords({\"time\": (\"time\", np.array([2000, 2050, 2100]))})\n",
    "    das.time.attrs[\"long_name\"] = \"time\"\n",
    "    das.time.attrs[\"units\"] = \"yr\"\n",
    "    daf = xr.concat(\n",
    "        [ds_list[i][\"base2000\"], ds_list[i][\"frag2050\"], ds_list[i][\"frag2100\"]],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "    daf = daf.assign_coords({\"time\": (\"time\", np.array([2000, 2050, 2100]))})\n",
    "    daf.time.attrs[\"long_name\"] = \"time\"\n",
    "    daf.time.attrs[\"units\"] = \"yr\"\n",
    "    dag = xr.concat(\n",
    "        [ds_list[i][\"base2000\"], ds_list[i][\"ffbd2050\"], ds_list[i][\"ffbd2100\"]],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "    dag = dag.assign_coords({\"time\": (\"time\", np.array([2000, 2050, 2100]))})\n",
    "    dag.time.attrs[\"long_name\"] = \"time\"\n",
    "    dag.time.attrs[\"units\"] = \"yr\"\n",
    "\n",
    "    dsnew = xr.concat([das, daf, dag], dim=\"nscenarios\")\n",
    "    dsnew = dsnew.assign_coords(\n",
    "        {\n",
    "            \"scenarios\": (\n",
    "                \"nscenarios\",\n",
    "                np.array([\"RCP4.5-SSP1\", \"RCP8.5-SSP3\", \"RCP8.5-SSP5\",], dtype=\"S\",),\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    dsnew.scenarios.attrs[\"long_name\"] = \"climate scenarios\"\n",
    "    dsnew = dsnew.to_dataset(name=var_list_abb[i])\n",
    "    dsnew[var_list_abb[i]].attrs[\"long_name\"] = var_list[i]  # variable attributes\n",
    "    dsnew.attrs = ds1.attrs  # copy global attributes\n",
    "    ds_list_new.append(dsnew)\n",
    "\n",
    "# merge into one dataset\n",
    "ds = xr.merge(ds_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-order shape of the data variables\n",
    "ds1_new = ds_list_new[0].transpose(\"nscenarios\", \"stations\", \"time\")\n",
    "ds2_new = ds_list_new[1].transpose(\"nscenarios\", \"stations\", \"time\")\n",
    "ds3_new = ds_list_new[2].transpose(\"nscenarios\", \"stations\", \"time\")\n",
    "ds = ds.transpose(\"nscenarios\", \"stations\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the xarray dataset, best practice is to have as many as possible bold dimensions (dimension == coordinate name).\n",
    "# in this way, the Front-End can access the variable directly without having to index the variable first\n",
    "\n",
    "ds\n",
    "# ds[\"scenarios\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c90980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new .nc files\n",
    "ds1_new.to_netcdf(path=str(ds_path1).replace(\".nc\", \"_CF.nc\"))\n",
    "ds2_new.to_netcdf(path=str(ds_path2).replace(\".nc\", \"_CF.nc\"))\n",
    "ds3_new.to_netcdf(path=str(ds_path3).replace(\".nc\", \"_CF.nc\"))\n",
    "ds.to_netcdf(path=ds_dir.joinpath(ds_out_file + \"_CF.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681119d6",
   "metadata": {},
   "source": [
    "### Check CF compliancy altered NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_path1).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cfc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_path1).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_path2).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_path2).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=str(ds_path3).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=str(ds_path3).replace(\".nc\", \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1883748",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "# check altered CF compliancy\n",
    "\n",
    "check_compliancy(testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save altered CF compliancy\n",
    "save_compliancy(\n",
    "    cap, testfile=ds_dir.joinpath(ds_out_file + \"_CF.nc\"), working_dir=CF_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615f75a",
   "metadata": {},
   "source": [
    "### write data to Zarr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to zarr in write mode (to overwrite if exists)\n",
    "ds.to_zarr(ds_dir.joinpath(\"%s.zarr\" % ds_out_file), mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f071d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d52b8dfbdab1c939c3c4b10b0d762f4c8139583e350f28e123ee37db8f80dd50"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
